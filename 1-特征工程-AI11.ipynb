{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T11:12:51.106394Z",
     "start_time": "2025-06-19T11:12:48.335815Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 DictVectorizer示例：将字典列表转换为特征矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T11:12:53.135546Z",
     "start_time": "2025-06-19T11:12:53.129671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名称: ['城市=上海' '城市=北京' '城市=深圳' '温度' '湿度' '风力=中' '风力=弱' '风力=强']\n",
      "特征矩阵:\n",
      " [[  0.   1.   0. 100.  60.   0.   0.   1.]\n",
      " [  1.   0.   0.  90.  70.   1.   0.   0.]\n",
      " [  0.   0.   1. 110.  80.   0.   1.   0.]]\n",
      "特征矩阵形状: (3, 8)\n",
      "特征类型 <class 'numpy.ndarray'>\n",
      "\n",
      "转换回的字典数据:\n",
      "{'城市=北京': 1.0, '温度': 100.0, '湿度': 60.0, '风力=强': 1.0}\n",
      "{'城市=上海': 1.0, '温度': 90.0, '湿度': 70.0, '风力=中': 1.0}\n",
      "{'城市=深圳': 1.0, '温度': 110.0, '湿度': 80.0, '风力=弱': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizer示例：将字典列表转换为特征矩阵\n",
    "\n",
    "# 创建一个字典列表，每个字典代表一个样本\n",
    "data = [\n",
    "    {'城市': '北京', '温度': 100, '湿度': 60, '风力': '强'},\n",
    "    {'城市': '上海', '温度': 90, '湿度': 70, '风力': '中'},\n",
    "    {'城市': '深圳', '温度': 110, '湿度': 80, '风力': '弱'}\n",
    "]\n",
    "\n",
    "# 初始化DictVectorizer，设置sparse=False返回密集矩阵而非稀疏矩阵\n",
    "#Sparse – Whether transform should produce scipy. sparse matrices\n",
    "# if Sparse is False, This returns a scipy sparse matrix (saves memory for large data).\n",
    "dict_vec = DictVectorizer(sparse=False)\n",
    "\n",
    "# 转换数据\n",
    "#fit_transform 's parameters: X – Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).\n",
    "\"\"\"\n",
    "fit_transform(X) \n",
    ".fit(data) → Look at all the keys in your dicts to learn what features exist.\n",
    "\n",
    ".transform(data) → Actually convert the dicts into a numeric matrix.\n",
    "\"\"\"\n",
    "feature_matrix = dict_vec.fit_transform(data)\n",
    "\n",
    "# 查看特征名称\n",
    "# get_feature_names_out() – Get output feature names for transformation\n",
    "feature_names = dict_vec.get_feature_names_out()\n",
    "\n",
    "print(\"特征名称:\", feature_names)\n",
    "print(\"特征矩阵:\\n\", feature_matrix)\n",
    "print(\"特征矩阵形状:\", feature_matrix.shape)\n",
    "print('特征类型',type(feature_matrix))\n",
    "\n",
    "# 将特征矩阵转换回字典形式\n",
    "# inverse_transform:Transform array or sparse matrix X back to feature mappings\n",
    "original_data = dict_vec.inverse_transform(feature_matrix)\n",
    "print(\"\\n转换回的字典数据:\")\n",
    "for item in original_data:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CountVectorizer英文示例：将英文文本转换为词频矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:32:10.600003Z",
     "start_time": "2025-06-19T12:32:10.522368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "英文词汇表: ['an' 'application' 'artificial' 'branch' 'deep' 'important'\n",
      " 'intelligence' 'is' 'language' 'learning' 'machine' 'method' 'natural'\n",
      " 'of' 'processing']\n",
      "英文词频矩阵:\n",
      " [[0 0 1 1 0 0 1 1 0 1 1 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 2 1 1 0 1 0]\n",
      " [1 1 1 0 0 1 1 1 1 0 0 0 1 1 1]]\n",
      "英文矩阵形状: (3, 15)\n",
      "\n",
      "文档-词条矩阵解释:\n",
      "文档 1: Machine learning is a branch of artificial intelligence\n",
      "包含的词条:\n",
      "  - 'artificial' 出现 1 次\n",
      "  - 'branch' 出现 1 次\n",
      "  - 'intelligence' 出现 1 次\n",
      "  - 'is' 出现 1 次\n",
      "  - 'learning' 出现 1 次\n",
      "  - 'machine' 出现 1 次\n",
      "  - 'of' 出现 1 次\n",
      "\n",
      "文档 2: Deep learning is a method of machine learning\n",
      "包含的词条:\n",
      "  - 'deep' 出现 1 次\n",
      "  - 'is' 出现 1 次\n",
      "  - 'learning' 出现 2 次\n",
      "  - 'machine' 出现 1 次\n",
      "  - 'method' 出现 1 次\n",
      "  - 'of' 出现 1 次\n",
      "\n",
      "文档 3: Natural language processing is an important application of artificial intelligence\n",
      "包含的词条:\n",
      "  - 'an' 出现 1 次\n",
      "  - 'application' 出现 1 次\n",
      "  - 'artificial' 出现 1 次\n",
      "  - 'important' 出现 1 次\n",
      "  - 'intelligence' 出现 1 次\n",
      "  - 'is' 出现 1 次\n",
      "  - 'language' 出现 1 次\n",
      "  - 'natural' 出现 1 次\n",
      "  - 'of' 出现 1 次\n",
      "  - 'processing' 出现 1 次\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer英文示例：将英文文本转换为词频矩阵\n",
    "\n",
    "# 准备一些简单的英文文本数据\n",
    "english_texts = [\n",
    "    \"Machine learning is a branch of artificial intelligence\",\n",
    "    \"Deep learning is a method of machine learning\",\n",
    "    \"Natural language processing is an important application of artificial intelligence\"\n",
    "]\n",
    "\n",
    "# 初始化CountVectorizer,空格，标点符号，都认为是分隔符，单个字母，认为没有语义\n",
    "english_count_vec = CountVectorizer()\n",
    "print(english_count_vec )\n",
    "# 转换文本数据为词频矩阵\n",
    "# fit_transform : will execute both fit and transform on the data. fit will learn and split it into vocabulary; transform: For each sentence, counts how many times each vocabulary word appears\n",
    "english_X = english_count_vec.fit_transform(english_texts)\n",
    "\n",
    "# 获取特征名称（词汇表）\n",
    "# get_feature_names_out() – Get output feature names for transformation\n",
    "english_vocabulary = english_count_vec.get_feature_names_out()\n",
    "\n",
    "# 将稀疏矩阵转换为密集矩阵以便于显示\n",
    "#toarray make us able to see the matrix; otherwise, it's an output with two columns:\n",
    "# Coords(coordinate: row and column) and Values( how many times the word appears in the document)\n",
    "english_X_dense = english_X.toarray()\n",
    "\n",
    "print(\"英文词汇表:\", english_vocabulary)\n",
    "print(\"英文词频矩阵:\\n\", english_X_dense)\n",
    "print(\"英文矩阵形状:\", english_X_dense.shape)\n",
    "\n",
    "# 分析结果\n",
    "print(\"\\n文档-词条矩阵解释:\")\n",
    "# enumerate will give us both the index and the value of each item in the list\n",
    "for i, doc in enumerate(english_texts):\n",
    "    print(f\"文档 {i+1}: {doc}\")\n",
    "    print(\"包含的词条:\")\n",
    "    for j, term in enumerate(english_vocabulary):\n",
    "        if english_X_dense[i, j] > 0:\n",
    "            print(f\"  - '{term}' 出现 {english_X_dense[i, j]} 次\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T13:05:11.200350Z",
     "start_time": "2025-06-19T13:05:11.179742Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/nt/z2_pm9xj1bl1zz8q88mshrpc0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认分词:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.374 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认模式: 我 爱 北京 天安门 ， 天安门 上 太阳升 。\n"
     ]
    }
   ],
   "source": [
    "# jieba分词示例\n",
    "\n",
    "# 准备一些中文文本\n",
    "text = \"我爱北京天安门，天安门上太阳升。\"\n",
    "\n",
    "# 默认分词\n",
    "print(\"默认分词:\") #jieba分词内部有很多分词算法\n",
    "#\n",
    "\"\"\"The main function that segments an entire sentence that contains Chinese characters into separated words.\n",
    "Parameter:\n",
    "sentence: The str(unicode) to be segmented.\n",
    "cut_all: Model type. True for full pattern, False for accurate pattern.\"\"\"\n",
    "seg_list = jieba.cut(text, cut_all=False) #返回的是迭代器\n",
    "print(\"默认模式: \" + \" \".join(seg_list))\n",
    "\n",
    "# # 全模式分词\n",
    "# print(\"\\n全模式分词:\")\n",
    "# seg_list = jieba.cut(text, cut_all=True)\n",
    "# print(\"全模式: \" + \"/ \".join(seg_list))\n",
    "\n",
    "# # 搜索引擎模式\n",
    "# print(\"\\n搜索引擎模式:\")\n",
    "# seg_list = jieba.cut_for_search(text)\n",
    "# print(\"搜索引擎模式: \" + \"/ \".join(seg_list))\n",
    "\n",
    "# # 添加自定义词典\n",
    "# print(\"\\n添加自定义词典:\")\n",
    "# jieba.add_word(\"天安门上\")\n",
    "# seg_list = jieba.cut(text, cut_all=False)\n",
    "# print(\"添加自定义词典后: \" + \"/ \".join(seg_list))\n",
    "\n",
    "# # 词性标注\n",
    "# print(\"\\n词性标注:\")\n",
    "# import jieba.posseg as pseg\n",
    "# words = pseg.cut(text)\n",
    "# print(\"词性标注结果:\")\n",
    "# for word, flag in words:\n",
    "#     print(f\"{word} ({flag})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T13:27:57.492338Z",
     "start_time": "2025-06-19T13:27:57.485116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['机器 学习 是 人工智能 的 一个 分支', '深度 学习 是 机器 学习 的 一种 方法', '自然语言 处理 是 人工智能 的 重要 应用']\n",
      "词汇表: ['机器学习是人工智能的一个分支' '深度学习是机器学习的一种方法' '自然语言处理是人工智能的重要应用']\n",
      "\n",
      " <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 3 stored elements and shape (3, 3)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 2)\t1\n",
      "词频矩阵:\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "矩阵形状: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer示例：将文本转换为词频矩阵\n",
    "\n",
    "# 准备一些简单的中文文本数据\n",
    "texts = [\n",
    "    \"机器学习是人工智能的一个分支\",\n",
    "    \"深度学习是机器学习的一种方法\",\n",
    "    \"自然语言处理是人工智能的重要应用\"\n",
    "]\n",
    "\n",
    "segmented_texts = [\" \".join(jieba.cut(text, cut_all=False)) for text in texts]\n",
    "\n",
    "print(segmented_texts)\n",
    "# 初始化CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "# 转换文本数据为词频矩阵\n",
    "X = count_vec.fit_transform(texts)\n",
    "# 获取特征名称（词汇表）\n",
    "vocabulary = count_vec.get_feature_names_out()\n",
    "# 将稀疏矩阵转换为密集矩阵以便于显示\n",
    "X_dense = X.toarray()\n",
    "\n",
    "#using CountVectorizer without applying jieba segmentation first. \n",
    "# That means it treats each entire sentence as one word (token), \n",
    "# because Chinese does not have spaces like English — so no natural token boundaries.\n",
    "# each full sentence is treated as a single token (like one long English word).\n",
    "\n",
    "print(\"词汇表:\", vocabulary)\n",
    "print('\\n',X)\n",
    "print(\"词频矩阵:\\n\", X_dense)\n",
    "print(\"矩阵形状:\", X_dense.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T03:33:33.170134Z",
     "start_time": "2025-06-20T03:33:33.136529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用jieba分词的CountVectorizer:\n",
      "jieba分词词汇表: ['人工智能' '学习' '是' '机器' '的']\n",
      "jieba分词词频矩阵:\n",
      " [[1 1 1 1 1]\n",
      " [0 2 1 1 1]\n",
      " [1 0 1 0 1]]\n",
      "jieba分词矩阵形状: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# 使用jieba分词器处理中文\n",
    "print(\"\\n使用jieba分词的CountVectorizer:\")\n",
    "#analyze function： If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "#min_df= minimum document frequency: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
    "# this means all terms that appear in less than 2 documents(2 documents rather than 2 words in a document)will be ignored.\n",
    "\n",
    "count_vec_jieba = CountVectorizer(analyzer=lambda x: jieba.cut(x),min_df=2)\n",
    "# fit_transform Returns ------- X : array of shape (n_samples, n_features) Document-term matrix. so the returned matrix will be passed tp CountVectorizer as input.\n",
    "\n",
    "\"\"\"\n",
    "# In .fit_transform(), the method first calls _count_vocab()\n",
    "# Inside _count_vocab(), it calls self.build_analyzer()\n",
    "# self.build_analyzer() returns a function (_analyze with partial arguments)\n",
    "# _analyze() checks if an analyzer is provided (not None)\n",
    "in  _analyze(), we can see:\n",
    "if analyzer is not None:\n",
    "    doc = analyzer(doc)\n",
    "\n",
    "# The processed doc is then returned\n",
    "return doc\n",
    "\n",
    "  # Since analyzer is a lambda function, it gets called here\n",
    "    # The input doc (from texts in fit_transform) is passed into the lambda\n",
    "    # The lambda function processes the text (e.g., using jieba.cut) and returns the result\n",
    "\"\"\"\n",
    "X_jieba = count_vec_jieba.fit_transform(texts)\n",
    "\n",
    "vocabulary_jieba = count_vec_jieba.get_feature_names_out()\n",
    "#['机器 学习 是 人工智能 的 一个 分支', '深度 学习 是 机器 学习 的 一种 方法', '自然语言 处理 是 人工智能 的 重要 应用']\n",
    "\n",
    "print(\"jieba分词词汇表:\", vocabulary_jieba)\n",
    "print(\"jieba分词词频矩阵:\\n\", X_jieba.toarray())\n",
    "print(\"jieba分词矩阵形状:\", X_jieba.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:11:22.280429Z",
     "start_time": "2025-06-20T09:11:22.199041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前处理： 机器学习是人工智能的一个分支\n",
      "当前处理： 深度学习是机器学习的一种方法\n",
      "当前处理： 自然语言处理是人工智能的重要应用\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# In .fit_transform(), the method first calls _count_vocab()\\n# Inside _count_vocab(), it calls self.build_analyzer()\\n# self.build_analyzer() returns a function (_analyze with partial arguments)\\n# _analyze() checks if an analyzer is provided (not None)\\nin  _analyze(), we can see:\\nif analyzer is not None:\\n    doc = analyzer(doc)\\n\\n# The processed doc is then returned\\nreturn doc\\n\\n  # Since analyzer is a lambda function, it gets called here\\n    # The input doc (from texts in fit_transform) is passed into the lambda\\n    # The lambda function processes the text (e.g., using jieba.cut) and returns the result\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_analyzer(x):\n",
    "    print(\"当前处理：\", x)\n",
    "    return jieba.cut(x)\n",
    "\n",
    "vec = CountVectorizer(analyzer=my_analyzer)\n",
    "vec.fit_transform(texts)\n",
    "\n",
    "\"\"\"\n",
    "# In .fit_transform(), the method first calls _count_vocab()\n",
    "# Inside _count_vocab(), it calls self.build_analyzer()\n",
    "# self.build_analyzer() returns a function (_analyze with partial arguments)\n",
    "# _analyze() checks if an analyzer is provided (not None)\n",
    "# in _analyze(), we can see:\n",
    "if analyzer is not None:\n",
    "    doc = analyzer(doc)\n",
    "\n",
    "# The processed doc is then returned\n",
    "return doc\n",
    "\n",
    "# Since analyzer is my_analyzer function, it gets called here\n",
    "# The input doc (from texts in fit_transform) is passed into the my_analyzer\n",
    "# The my_analyzer function processes the text (e.g., using jieba.cut) and returns the result\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "say_hello() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msay_hello\u001b[39m(name):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHello, \u001b[39m\u001b[33m\"\u001b[39m, name)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m a=\u001b[43msay_hello\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msay_something\u001b[39m(words):\n\u001b[32m      6\u001b[39m    \u001b[38;5;28mprint\u001b[39m(words)\n",
      "\u001b[31mTypeError\u001b[39m: say_hello() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T10:59:29.679968Z",
     "start_time": "2025-06-20T10:59:29.668089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用jieba分词的TF-IDF处理:\n",
      "jieba分词词汇表: ['一个' '一种' '人工智能' '分支' '处理' '学习' '应用' '方法' '是' '机器' '深度' '的' '自然语言' '重要']\n",
      "jieba分词TF-IDF特征矩阵:\n",
      " [[0.47496141 0.         0.3612204  0.47496141 0.         0.3612204\n",
      "  0.         0.         0.28051986 0.3612204  0.         0.28051986\n",
      "  0.         0.        ]\n",
      " [0.         0.38955498 0.         0.         0.         0.5925332\n",
      "  0.         0.38955498 0.23007745 0.2962666  0.38955498 0.23007745\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.3311001  0.         0.43535684 0.\n",
      "  0.43535684 0.         0.25712876 0.         0.         0.25712876\n",
      "  0.43535684 0.43535684]]\n",
      "jieba分词矩阵形状: (3, 14)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF 主要目的：找出词频很高，但又具有独特性的单词（没有在很多文档中出现，仅在部分文档中出现达到了很高词频）\n",
    "# TF-IDF处理示例：将文本转换为TF-IDF特征矩阵\n",
    "# 使用jieba分词器进行TF-IDF处理\n",
    "print(\"\\n使用jieba分词的TF-IDF处理:\")\n",
    "tfidf_vec_jieba = TfidfVectorizer(analyzer=lambda x: jieba.cut(x))\n",
    "X_tfidf_jieba = tfidf_vec_jieba.fit_transform(texts)\n",
    "#Get output feature names for transformation.\n",
    "tfidf_vocabulary_jieba = tfidf_vec_jieba.get_feature_names_out()\n",
    "\n",
    "print(\"jieba分词词汇表:\", tfidf_vocabulary_jieba)\n",
    "# print('TF-IDF特征矩阵没有toarray',X_tfidf_jieba)\n",
    "\"\"\"\n",
    "texts=['机器学习是人工智能的一个分支', '深度学习是机器学习的一种方法', '自然语言处理是人工智能的重要应用']\n",
    "\"\"\"\n",
    "print(\"jieba分词TF-IDF特征矩阵:\\n\", X_tfidf_jieba.toarray())\n",
    "print(\"jieba分词矩阵形状:\", X_tfidf_jieba.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化示例:\n",
      "原始数据矩阵:\n",
      " [[ 1 -1  2]\n",
      " [ 2  0  0]\n",
      " [ 0  1 -1]\n",
      " [ 5  2  0]]\n",
      "数据矩阵形状: (4, 3)\n",
      "\n",
      "MinMaxScaler归一化后的数据(缩放到[0,1]):\n",
      " [[0.2        0.         1.        ]\n",
      " [0.4        0.33333333 0.33333333]\n",
      " [0.         0.66666667 0.        ]\n",
      " [1.         1.         0.33333333]]\n",
      "MinMaxScaler归一化后的数据(缩放到[-1,1]):\n",
      " [[-0.6        -1.          1.        ]\n",
      " [-0.2        -0.33333333 -0.33333333]\n",
      " [-1.          0.33333333 -1.        ]\n",
      " [ 1.          1.         -0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "# 导入归一化相关的库\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个简单的数据矩阵作为示例\n",
    "print(\"归一化示例:\")\n",
    "X = np.array([\n",
    "    [1, -1, 2],\n",
    "    [2, 0, 0],\n",
    "    [0, 1, -1],\n",
    "    [5, 2, 0]\n",
    "])\n",
    "print(\"原始数据矩阵:\\n\", X)\n",
    "print(\"数据矩阵形状:\", X.shape)\n",
    "\n",
    "# MinMaxScaler归一化：将数据缩放到[0,1]区间\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "print(\"\\nMinMaxScaler归一化后的数据(缩放到[0,1]):\\n\", X_minmax)\n",
    "\n",
    "# 使用MinMaxScaler(feature_range=(-1, 1)），也可以缩放到其他区间，例如[-1,1]\n",
    "min_max_scaler_custom = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_minmax_custom = min_max_scaler_custom.fit_transform(X)\n",
    "print(\"MinMaxScaler归一化后的数据(缩放到[-1,1]):\\n\", X_minmax_custom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StandardScaler标准化示例:\n",
      "原始数据矩阵:\n",
      " [[ 1. -1.  3.]\n",
      " [ 2.  4.  2.]\n",
      " [ 4.  6. -1.]]\n",
      "StandardScaler标准化后的数据:\n",
      " [[-1.06904497 -1.35873244  0.98058068]\n",
      " [-0.26726124  0.33968311  0.39223227]\n",
      " [ 1.33630621  1.01904933 -1.37281295]]\n",
      "标准化后的均值: [-1.48029737e-16  7.40148683e-17  7.40148683e-17]\n",
      "标准化后的标准差: [1. 1. 1.]\n",
      "\n",
      "原始数据的均值: [2.33333333 3.         1.33333333]\n",
      "原始数据的标准差: [1.24721913 2.94392029 1.69967317]\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler标准化：将数据转换为均值为0，标准差为1的分布\n",
    "print(\"\\nStandardScaler标准化示例:\")\n",
    "X_example = np.array([[1., -1., 3.],\n",
    "                      [2., 4., 2.],\n",
    "                      [4., 6., -1.]])\n",
    "print(\"原始数据矩阵:\\n\", X_example)\n",
    "\n",
    "# 应用StandardScaler进行标准化\n",
    "std_scaler = StandardScaler()\n",
    "X_std = std_scaler.fit_transform(X_example)\n",
    "print(\"StandardScaler标准化后的数据:\\n\", X_std)\n",
    "\n",
    "# 查看标准化后的均值和标准差\n",
    "print(\"标准化后的均值:\", X_std.mean(axis=0)) #无限接近0\n",
    "print(\"标准化后的标准差:\", X_std.std(axis=0))\n",
    "\n",
    "# 也可以查看原始数据的均值和标准差\n",
    "print(\"\\n原始数据的均值:\", X_example.mean(axis=0))\n",
    "print(\"原始数据的标准差:\", X_example.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SimpleImputer示例:\n",
      "包含缺失值的原始数据矩阵:\n",
      " [[nan  2.  3.]\n",
      " [ 4. nan  6.]\n",
      " [ 7.  8. nan]\n",
      " [nan nan  6.]]\n",
      "\n",
      "使用均值填充后的数据矩阵:\n",
      " [[5.5 2.  3. ]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  5. ]\n",
      " [5.5 5.  6. ]]\n",
      "\n",
      "使用中位数填充后的数据矩阵:\n",
      " [[5.5 2.  3. ]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  6. ]\n",
      " [5.5 5.  6. ]]\n",
      "\n",
      "使用常数0填充后的数据矩阵:\n",
      " [[0. 2. 3.]\n",
      " [4. 0. 6.]\n",
      " [7. 8. 0.]\n",
      " [0. 0. 6.]]\n",
      "\n",
      "使用最频繁值填充后的数据矩阵:\n",
      " [[4. 2. 3.]\n",
      " [4. 2. 6.]\n",
      " [7. 8. 6.]\n",
      " [4. 2. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# SimpleImputer用于处理缺失值\n",
    "print(\"\\nSimpleImputer示例:\")\n",
    "# 创建一个包含缺失值的数据矩阵\n",
    "X_missing = np.array([\n",
    "    [np.nan, 2, 3],\n",
    "    [4, np.nan, 6],\n",
    "    [7, 8, np.nan],\n",
    "    [np.nan, np.nan, 6]\n",
    "])\n",
    "print(\"包含缺失值的原始数据矩阵:\\n\", X_missing)\n",
    "\n",
    "# missing_values : default=np. nan\n",
    "# 使用均值策略填充缺失值\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "X_imputed_mean = mean_imputer.fit_transform(X_missing)\n",
    "print(\"\\n使用均值填充后的数据矩阵:\\n\", X_imputed_mean)\n",
    "\n",
    "# 使用中位数策略填充缺失值\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "X_imputed_median = median_imputer.fit_transform(X_missing)\n",
    "print(\"\\n使用中位数填充后的数据矩阵:\\n\", X_imputed_median)\n",
    "\n",
    "# 使用常数值填充缺失值\n",
    "constant_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_imputed_constant = constant_imputer.fit_transform(X_missing)\n",
    "print(\"\\n使用常数0填充后的数据矩阵:\\n\", X_imputed_constant)\n",
    "\n",
    "# 使用最频繁值填充缺失值,众数\n",
    "most_frequent_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_imputed_most_frequent = most_frequent_imputer.fit_transform(X_missing)\n",
    "print(\"\\n使用最频繁值填充后的数据矩阵:\\n\", X_imputed_most_frequent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方差阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:43:10.003542Z",
     "start_time": "2025-06-20T12:43:09.993970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VarianceThreshold示例:\n",
      "原始数据矩阵:\n",
      " [[0 2 0 3]\n",
      " [0 1 4 3]\n",
      " [0 1 1 3]]\n",
      "\n",
      "各特征的方差: [0.         0.22222222 2.88888889 0.        ]\n",
      "\n",
      "保留的特征索引: [2]\n",
      "保留的特征方差: [2.88888889]\n",
      "\n",
      "特征选择后的数据矩阵:\n",
      " [[0]\n",
      " [4]\n",
      " [1]]\n",
      "\n",
      "阈值为0.1时保留的特征索引: [1 2]\n",
      "阈值为0.1时特征选择后的数据矩阵:\n",
      " [[2 0]\n",
      " [1 4]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "# VarianceThreshold用于特征选择，去除方差低于阈值的特征\n",
    "print(\"\\nVarianceThreshold示例:\")\n",
    "\n",
    "# 创建一个示例数据矩阵\n",
    "X_variance = np.array([\n",
    "    [0, 2, 0, 3],\n",
    "    [0, 1, 4, 3],\n",
    "    [0, 1, 1, 3]\n",
    "])\n",
    "print(\"原始数据矩阵:\\n\", X_variance)\n",
    "\n",
    "# 计算每个特征的方差\n",
    "# The default is to compute the variance of the flattened array. \n",
    "feature_variances = np.var(X_variance, axis=0)\n",
    "print(\"\\n各特征的方差:\", feature_variances)\n",
    "\n",
    "# 使用方差阈值为0.8进行特征选择# 这将移除方差小于0.8的特征\n",
    "selector = VarianceThreshold(threshold=0.8)\n",
    "X_selected = selector.fit_transform(X_variance)\n",
    "# 显示保留的特征索引\n",
    "print(\"\\n保留的特征索引:\", selector.get_support(indices=True))\n",
    "print(\"保留的特征方差:\", feature_variances[selector.get_support()])\n",
    "print(\"\\n特征选择后的数据矩阵:\\n\", X_selected)\n",
    "# 尝试不同的阈值\n",
    "selector_low = VarianceThreshold(threshold=0.1)\n",
    "X_selected_low = selector_low.fit_transform(X_variance)\n",
    "print(\"\\n阈值为0.1时保留的特征索引:\", selector_low.get_support(indices=True))\n",
    "print(\"阈值为0.1时特征选择后的数据矩阵:\\n\", X_selected_low)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:57:14.643649Z",
     "start_time": "2025-06-20T12:57:13.883199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原始数据方差: [ 2.88888889  4.66666667 13.55555556  8.22222222]\n",
      "\n",
      "原始数据方差和: 29.333333333333336\n",
      "完整PCA示例:\n",
      "原始数据:\n",
      " [[2 8 4 5]\n",
      " [6 3 0 8]\n",
      " [5 4 9 1]]\n",
      "\n",
      "数据形状: (3, 4)\n",
      "\n",
      "PCA转换后的数据:\n",
      " [[-9.33473422e-16  3.82970843e+00  3.58835645e-16]\n",
      " [-5.74456265e+00 -1.91485422e+00  3.58835645e-16]\n",
      " [ 5.74456265e+00 -1.91485422e+00  3.58835645e-16]]\n",
      "\n",
      "转换后数据形状: (3, 3)\n",
      "\n",
      "转换后数据方差: [2.20000000e+01 7.33333333e+00 1.62057690e-63]\n",
      "\n",
      "转换后数据方差和: 29.333333333333332\n",
      "\n",
      "各主成分解释的方差比例: [7.50000000e-01 2.50000000e-01 4.38964841e-33]\n",
      "累计解释的方差比例: 1.0\n",
      "\n",
      "\n",
      "降维到2个主成分的PCA示例:\n",
      "原始数据:\n",
      " [[2 8 4 5]\n",
      " [6 3 0 8]\n",
      " [5 4 9 1]]\n",
      "\n",
      "数据形状: (3, 4)\n",
      "\n",
      "PCA转换后的数据:\n",
      " [[-9.33473422e-16  3.82970843e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]]\n",
      "\n",
      "转换后数据形状: (3, 2)\n",
      "\n",
      "转换后数据方差: [22.          7.33333333]\n",
      "\n",
      "转换后数据方差和: 29.333333333333332\n",
      "\n",
      "各主成分解释的方差比例: [0.75 0.25]\n",
      "累计解释的方差比例: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 定义一个理解PCA的示例函数\n",
    "def understand_pca(data, n_components=None):\n",
    "    \"\"\"\n",
    "    通过示例理解PCA主成分分析\n",
    "    \n",
    "    参数:\n",
    "    data: 输入数据矩阵\n",
    "    n_components: 要保留的主成分比例，如果是0到1之间的浮点数，表示保留的主成分解释的方差比例。如果是整数，表示保留的列数量。\n",
    "    \n",
    "    返回:\n",
    "    pca模型、转换后的数据、解释方差比例\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # 创建PCA模型\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # 拟合数据并转换\n",
    "    transformed_data = pca.fit_transform(data)\n",
    "    \n",
    "    # 输出原始数据和结果\n",
    "    print(\"原始数据:\\n\", data)\n",
    "    print(\"\\n数据形状:\", data.shape)\n",
    "    \n",
    "    # 输出PCA的结果\n",
    "    print(\"\\nPCA转换后的数据:\\n\", transformed_data)\n",
    "    print(\"\\n转换后数据形状:\", transformed_data.shape)\n",
    "    #输出transformed_data的方差\n",
    "    print(\"\\n转换后数据方差:\", np.var(transformed_data, axis=0))\n",
    "    #输出transformed_data的方差和\n",
    "    print(\"\\n转换后数据方差和:\", np.var(transformed_data, axis=0).sum())\n",
    "    \n",
    "    # 主成分解释的方差比例\n",
    "    print(\"\\n各主成分解释的方差比例:\", pca.explained_variance_ratio_)\n",
    "    print(\"累计解释的方差比例:\", np.sum(pca.explained_variance_ratio_))\n",
    "    \n",
    "    \n",
    "    return pca, transformed_data, pca.explained_variance_ratio_\n",
    "\n",
    "# 使用上下文中的数据\n",
    "X = np.array([[2, 8, 4, 5],\n",
    "              [6, 3, 0, 8],\n",
    "              [5, 4, 9, 1]])\n",
    "\n",
    "#输出X的方差\n",
    "print(\"\\n原始数据方差:\", np.var(X, axis=0))\n",
    "#输出X的方差和\n",
    "print(\"\\n原始数据方差和:\", np.var(X, axis=0).sum())\n",
    "\n",
    "# 调用函数演示PCA\n",
    "print(\"完整PCA示例:\")\n",
    "pca_full, data_full, var_ratio_full = understand_pca(X)\n",
    "\n",
    "# 降维到2个主成分\n",
    "print(\"\\n\\n降维到2个主成分的PCA示例:\")\n",
    "pca_2d, data_2d, var_ratio_2d = understand_pca(X, n_components=0.9)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
